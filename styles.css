import pandas as pd
from rapidfuzz import fuzz

# Load Excel
df = pd.read_excel("your_file.xlsx")  # Replace with your Excel file

results = []
scores_log = []  # For audit: all scores

# Process each Account no group
for account, group_df in df.groupby('Account no'):
    parties = group_df['Party name'].tolist()
    assigned_group = {}  # Map party index to (group_name, score)

    # Store all scores for auditing
    for i, party_i in enumerate(parties):
        for j, party_j in enumerate(parties):
            if i == j:
                continue
            score = fuzz.ratio(party_i, party_j)
            scores_log.append([account, party_i, party_j, score])

            if score > 80:
                # Prioritize 100 match as group leaders
                if fuzz.ratio(party_i, party_i) == 100:
                    group_name = party_i
                elif fuzz.ratio(party_j, party_j) == 100:
                    group_name = party_j
                else:
                    group_name = party_i  # fallback to first

                # Update if new or better match
                if i not in assigned_group or assigned_group[i][1] < score:
                    assigned_group[i] = (group_name, score)
                if j not in assigned_group or assigned_group[j][1] < score:
                    assigned_group[j] = (group_name, score)

    # Assign group or mark as self group
    for idx, party in enumerate(parties):
        if idx in assigned_group:
            group_name, score = assigned_group[idx]
        else:
            group_name, score = party, 100
        results.append([account, party, group_name, score])

# Create output DataFrames
final_df = pd.DataFrame(results, columns=['Account no', 'Party name', 'group_name', 'score'])
audit_df = pd.DataFrame(scores_log, columns=['Account no', 'Party 1', 'Party 2', 'Score'])

# Count number of unique groups per account
group_counts = final_df.groupby('Account no')['group_name'].nunique().reset_index()
group_counts.columns = ['Account no', 'Number of Groups']

# Save to Excel with audit and group count sheet
with pd.ExcelWriter("grouped_output.xlsx") as writer:
    final_df.to_excel(writer, index=False, sheet_name="Grouped Output")
    audit_df.to_excel(writer, index=False, sheet_name="Audit Scores")
    group_counts.to_excel(writer, index=False, sheet_name="Group Count")

print("Saved to grouped_output.xlsx with audit and group count sheets")







import pandas as pd
from rapidfuzz import fuzz

# Load Excel
df = pd.read_excel("your_file.xlsx")  # Replace with your Excel file

results = []
scores_log = []  # For audit: all scores

# Process each Account no group
for account, group_df in df.groupby('Account no'):
    parties = group_df['Party name'].tolist()
    assigned_group = {}  # Map party index to (group_name, score)

    # Store all scores for auditing
    for i, party_i in enumerate(parties):
        for j, party_j in enumerate(parties):
            if i == j:
                continue
            score = fuzz.ratio(party_i, party_j)
            scores_log.append([account, party_i, party_j, score])

            if score > 80:
                # Prioritize 100 match as group leaders
                if fuzz.ratio(party_i, party_i) == 100:
                    group_name = party_i
                elif fuzz.ratio(party_j, party_j) == 100:
                    group_name = party_j
                else:
                    group_name = party_i  # fallback to first

                # Update if new or better match
                if i not in assigned_group or assigned_group[i][1] < score:
                    assigned_group[i] = (group_name, score)
                if j not in assigned_group or assigned_group[j][1] < score:
                    assigned_group[j] = (group_name, score)

    group_name_to_number = {}
    next_group_number = 1

    # Assign group or mark as self group
    for idx, party in enumerate(parties):
        if idx in assigned_group:
            group_name, score = assigned_group[idx]
        else:
            group_name, score = party, 100

        if group_name not in group_name_to_number:
            group_name_to_number[group_name] = next_group_number
            next_group_number += 1

        group_number = group_name_to_number[group_name]
        results.append([account, party, group_name, score, group_number])

# Create output DataFrames
final_df = pd.DataFrame(results, columns=['Account no', 'Party name', 'group_name', 'score', 'group_number'])
audit_df = pd.DataFrame(scores_log, columns=['Account no', 'Party 1', 'Party 2', 'Score'])

# Count number of unique groups per account
group_counts = final_df.groupby('Account no')['group_name'].nunique().reset_index()
group_counts.columns = ['Account no', 'Number of Groups']

# Save to Excel with audit and group count sheet
with pd.ExcelWriter("grouped_output.xlsx") as writer:
    final_df.to_excel(writer, index=False, sheet_name="Grouped Output")
    audit_df.to_excel(writer, index=False, sheet_name="Audit Scores")
    group_counts.to_excel(writer, index=False, sheet_name="Group Count")

print("Saved to grouped_output.xlsx with audit and group count sheets")







import pandas as pd
from rapidfuzz import fuzz

# Load Excel file
excel_file = "your_file.xlsx"  # Update with your actual file name
df = pd.read_excel(excel_file)

# Set similarity threshold
SIMILARITY_THRESHOLD = 80

all_results = []

# Go through each account separately
for account_no, account_group in df.groupby('Account no'):
    party_names = account_group['Party name'].tolist()
    num_parties = len(party_names)

    # Step 1: Create similarity matrix
    similarity = [[0] * num_parties for _ in range(num_parties)]
    for i in range(num_parties):
        for j in range(i + 1, num_parties):
            score = fuzz.ratio(party_names[i], party_names[j])
            similarity[i][j] = score
            similarity[j][i] = score

    # Step 2: Group parties using overlap-friendly logic
    groups = []
    added_to_group = [False] * num_parties

    for i in range(num_parties):
        group = set([i])
        for j in range(num_parties):
            if i != j and similarity[i][j] > SIMILARITY_THRESHOLD:
                group.add(j)
        if group not in groups:
            groups.append(group)

    # Step 3: Assign group name and number
    seen = set()
    unique_groups = []
    for group in groups:
        frozen = frozenset(group)
        if frozen not in seen:
            seen.add(frozen)
            unique_groups.append(list(group))

    for group_num, group_indices in enumerate(unique_groups, start=1):
        group_parties = [party_names[i] for i in group_indices]

        # Group name: pick the one with perfect match (100%) to any other
        group_name = None
        for i in group_indices:
            for j in group_indices:
                if i != j and similarity[i][j] == 100:
                    group_name = party_names[i]
                    break
            if group_name:
                break
        if not group_name:
            group_name = party_names[group_indices[0]]  # fallback

        for i in group_indices:
            score = 100 if party_names[i] == group_name else fuzz.ratio(party_names[i], group_name)
            all_results.append([
                account_no,
                party_names[i],
                group_name,
                score,
                group_num
            ])

# Final output DataFrame
output_df = pd.DataFrame(all_results, columns=[
    'Account no', 'Party name', 'group_name', 'score', 'group_number'
])

# Save to Excel
output_df.to_excel("grouped_output_simple.xlsx", index=False)
print("Saved grouped output to grouped_output_simple.xlsx")


import pandas as pd
from rapidfuzz import fuzz
from collections import defaultdict, deque

# Load all sheets
excel_file = "your_file.xlsx"
sheets = pd.read_excel(excel_file, sheet_name=None)

# Sheet to process
sheet_to_group = "Sheet1"  # Change to the actual sheet name
df = sheets[sheet_to_group]

# Set similarity threshold
SIMILARITY_THRESHOLD = 80

# Prepare new columns to store results
df['group_name'] = None
df['group_number'] = None
df['score'] = None

# Group processing
for account_no, account_group in df.groupby('Account no'):
    party_names = account_group['Party name'].tolist()
    num_parties = len(party_names)

    # Step 1: Build graph of similarities
    graph = defaultdict(set)
    for i in range(num_parties):
        for j in range(i + 1, num_parties):
            score = fuzz.token_set_ratio(party_names[i], party_names[j])
            if score > SIMILARITY_THRESHOLD:
                graph[i].add(j)
                graph[j].add(i)

    # Step 2: Find connected components (transitive groups)
    visited = [False] * num_parties
    unique_groups = []

    for i in range(num_parties):
        if not visited[i]:
            queue = deque([i])
            group = []
            while queue:
                node = queue.popleft()
                if not visited[node]:
                    visited[node] = True
                    group.append(node)
                    queue.extend(graph[node])
            unique_groups.append(group)

    # Step 3: Assign results directly into df
    for group_num, group_indices in enumerate(unique_groups, start=1):
        group_parties = [party_names[i] for i in group_indices]

        # Determine group name
        group_name = None
        for i in group_indices:
            for j in group_indices:
                if i != j and fuzz.token_set_ratio(party_names[i], party_names[j]) == 100:
                    group_name = party_names[i]
                    break
            if group_name:
                break
        if not group_name:
            group_name = party_names[group_indices[0]]

        # Update original DataFrame rows
        for i in group_indices:
            row_index = account_group.index[i]  # Get index in original df
            score = 100 if party_names[i] == group_name else fuzz.token_set_ratio(party_names[i], group_name)
            df.loc[row_index, 'group_name'] = group_name
            df.loc[row_index, 'group_number'] = group_num
            df.loc[row_index, 'score'] = score

# Step: Summary of group counts per account
summary_df = df[['Account no', 'group_number']].drop_duplicates()
group_counts = summary_df.groupby('Account no')['group_number'].nunique().reset_index()
group_counts.columns = ['Account no', 'number_of_groups']

# Update sheets dictionary
sheets[sheet_to_group] = df
sheets['Group Summary'] = group_counts

# Save all sheets to new file
with pd.ExcelWriter("grouped_output_with_all_sheets.xlsx", engine='openpyxl') as writer:
    for sheet_name, sheet_df in sheets.items():
        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)

print("Saved grouped output with all sheets and summary to grouped_output_with_all_sheets.xlsx")







# Determine group name as the one matched to most others >= threshold
        best_match_count = -1
        group_name = group_parties[0]
        for candidate in group_parties:
            match_count = sum(1 for other in group_parties if candidate != other and fuzz.token_set_ratio(candidate, other) >= SIMILARITY_THRESHOLD)
            if match_count > best_match_count:
                best_match_count = match_count
                group_name = candidate



import pandas as pd
from rapidfuzz import fuzz
from collections import defaultdict, deque

# Load all sheets
excel_file = "your_file.xlsx"
sheets = pd.read_excel(excel_file, sheet_name=None)

# Sheet to process
sheet_to_group = "Sheet1"  # Change to the actual sheet name
df = sheets[sheet_to_group]

# Set similarity threshold
SIMILARITY_THRESHOLD = 80

# Prepare new columns to store results
df['group_name'] = None
df['group_number'] = None
df['score'] = None

# Group processing
for account_no, account_group in df.groupby('Account no'):
    # Filter out rows with blank 'Party name' for processing
    account_group_nonblank = account_group[account_group['Party name'].notna() & (account_group['Party name'].str.strip() != '')]
    party_names = account_group_nonblank['Party name'].tolist()
    num_parties = len(party_names)

    if num_parties == 0:
        continue

    # Step 1: Build full similarity matrix
    similarity = [[0] * num_parties for _ in range(num_parties)]
    for i in range(num_parties):
        for j in range(i + 1, num_parties):
            score = fuzz.token_set_ratio(party_names[i], party_names[j])
            similarity[i][j] = score
            similarity[j][i] = score

    # Step 2: Build graph only if all pairwise scores in group meet threshold
    graph = defaultdict(set)
    for i in range(num_parties):
        for j in range(i + 1, num_parties):
            if similarity[i][j] >= SIMILARITY_THRESHOLD:
                graph[i].add(j)
                graph[j].add(i)

    # Step 3: Find connected components (transitive groups)
    visited = [False] * num_parties
    raw_groups = []
    for i in range(num_parties):
        if not visited[i]:
            queue = deque([i])
            group = []
            while queue:
                node = queue.popleft()
                if not visited[node]:
                    visited[node] = True
                    group.append(node)
                    queue.extend(graph[node])
            raw_groups.append(group)

    # Step 4: Filter groups to ensure all pairwise scores >= threshold
    filtered_groups = []
    for group in raw_groups:
        valid = True
        for i in range(len(group)):
            for j in range(i + 1, len(group)):
                if similarity[group[i]][group[j]] < SIMILARITY_THRESHOLD:
                    valid = False
                    break
            if not valid:
                break
        if valid:
            filtered_groups.append(group)
        else:
            # Split invalid group into individual single-member groups
            for idx in group:
                filtered_groups.append([idx])

    # Step 5: Assign results directly into df
    for group_num, group_indices in enumerate(filtered_groups, start=1):
        group_parties = [party_names[i] for i in group_indices]

        # Determine group name as the one matched to most others >= threshold
        best_match_count = -1
        group_name = group_parties[0]
        for candidate in group_parties:
            match_count = sum(1 for other in group_parties if candidate != other and fuzz.token_set_ratio(candidate, other) >= SIMILARITY_THRESHOLD)
            if match_count > best_match_count:
                best_match_count = match_count
                group_name = candidate

        # Update original DataFrame rows
        for i in group_indices:
            row_index = account_group_nonblank.index[i]  # Get index in original df
            score = 100 if party_names[i] == group_name else fuzz.token_set_ratio(party_names[i], group_name)
            df.loc[row_index, 'group_name'] = group_name
            df.loc[row_index, 'group_number'] = group_num
            df.loc[row_index, 'score'] = score

# Step: Summary of group counts per account
summary_df = df[['Account no', 'group_number']].drop_duplicates()
group_counts = summary_df.groupby('Account no')['group_number'].nunique().reset_index()
group_counts.columns = ['Account no', 'number_of_groups']

# Update sheets dictionary
sheets[sheet_to_group] = df
sheets['Group Summary'] = group_counts

# Save all sheets to new file
with pd.ExcelWriter("grouped_output_with_all_sheets.xlsx", engine='openpyxl') as writer:
    for sheet_name, sheet_df in sheets.items():
        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)

print("Saved grouped output with all sheets and summary to grouped_output_with_all_sheets.xlsx")
